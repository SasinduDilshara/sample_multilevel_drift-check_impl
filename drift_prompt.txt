Of course. I've updated your prompt to produce a descriptive, paragraph-based report, removed the organization-level documentation as requested, and tailored the instructions to generate an output similar to your provided sample.

Here is the revised prompt:

-----

# Multi-Level Documentation Drift Analysis and Reporting

## **Objective**

As an expert software engineer, your task is to analyze a given codebase to detect "documentation drift"â€”discrepancies between the implementation and its corresponding documentation. You will examine multiple levels of documentation, from high-level project and component guides down to inline code comments. Based on this analysis, you will generate a clear, descriptive, and insightful report written in prose. This report should summarize the overall health of the project's documentation and detail specific issues within its components, providing actionable feedback in a narrative format.

-----

## **Core Concepts**

### **Terminology**

1.  **Program**: The actual source code containing the implementation and its documentation.
2.  **Entity**: Any identifiable code construct, such as a function, class, method, module, or component.
3.  **Implementation**: The executable logic of an entity, separate from any comments or documentation.
4.  **Project-Level Documentation**: High-level documents that describe the entire project, such as `README.md` files, requirements specifications, and architectural overviews.
5.  **Component-Level Documentation**: Documents that describe specific modules, packages, or major features within the project.
6.  **API Documentation**: Formal documentation generated from code comments that follow language-specific conventions (e.g., Javadoc `/** */`, Python docstrings `"""`, Rust `///`).
7.  **Inline Comments**: Comments written directly within the code logic to explain implementation details (e.g., `//`, `#`).
8.  **Drift**: A mismatch or inconsistency between the code's implementation and any level of its documentation.

### **Documentation Priority Hierarchy**

When conflicts arise between different levels of documentation, the system will prioritize them in the following order (from highest to lowest):

1.  **Project-Level Documentation** (Highest Priority)
2.  **Component-Level Documentation**
3.  **API Documentation**
4.  **Inline Comments** (Lowest Priority)

Conflicts between these levels should be noted in the analysis.

-----

## **Input Structure**

You will receive the project context in the following structure:

```xml
<analysis_context>
  <project_docs>{{PROJECT_DOCUMENTATION}}</project_docs>
  <component_docs>{{COMPONENT_DOCUMENTATION}}</component_docs>
  <source_files>{{SOURCE_FILES}}</source_files>
</analysis_context>
```

-----

## **Analysis Process (Internal Steps)**

You will perform the following analysis internally to gather the insights needed for the final report.

### **1. Context Analysis**

  * Parse and comprehend the **project-level documentation** (requirements, READMEs) to understand the project's goals and specifications.
  * Parse and comprehend the **component-level documentation** to understand the intended purpose and design of individual modules.
  * Establish the documentation hierarchy and identify any high-level conflicts between project and component docs.

### **2. Source Code Analysis**

  * For each source file, automatically detect the programming language and its specific documentation conventions (e.g., `/** */` vs. `"""`).
  * Identify all code entities (functions, classes, etc.) and extract their associated documentation (API docs and inline comments).

### **3. Multi-Level Drift Detection**

  * For each code entity, systematically compare its implementation against all relevant documentation layers to identify drift.
      * **Project-Level Drift**: Does the implemented feature align with the project's `README` and requirements?
      * **Component-Level Drift**: Does the entity's behavior match the description in its component documentation?
      * **API Documentation Drift**: Are the parameters, return values, and described behavior in the API docs accurate?
      * **Inline Comment Drift**: Do the inline comments correctly reflect the logic they are intended to explain?

### **4. Intelligent Categorization**

  * Internally categorize each detected issue by its **severity** (Critical, Major, Minor), **type** (Missing, Outdated, Incorrect, Conflicting), and the **documentation level** it occurred at. This categorization will inform the tone and focus of the final report.

-----

## **Output Generation Guidelines**

Generate a comprehensive report in markdown format. The report must be structured as a set of descriptive, rich-content paragraphs, using the sample provided as a stylistic guide.

### **1. Main Project Summary**

  * Begin the report with a primary heading, such as `### Project Summary`.
  * In this section, provide a holistic overview of the project's documentation health.
  * Assign an overall rating (e.g., **"Good"**, **"Fair"**, **"Needs Improvement"**) and state it clearly.
  * Summarize the key findings, highlighting both strengths (e.g., "sound resource modeling," "secure authentication") and critical deficiencies (e.g., "lack of standardized error objects," "omission of recommended REST headers").
  * Conclude with a statement on the overall impact and suggest a course of action.

### **2. Detailed Component Analysis**

  * Follow the summary with a clear separator (`***`) and a new section with a heading like `### Component Analysis` or `### API Endpoint Analysis`.
  * For each major component, endpoint, or file analyzed, create a dedicated sub-heading (e.g., `**MyService - REST Endpoint**`).
  * Under each sub-heading, write a detailed paragraph describing the findings for that specific component.
  * Include its individual rating (e.g., "rated **'Fair'**") and explain the reasoning.
  * Describe what the component does well (e.g., "adheres to most RESTful principles") and detail the specific documentation drift issues found (e.g., "lacks explicit versioning," "omits important response headers," "documentation is missing contact and support information").

### **3. Tone and Style**

  * Write in a professional, clear, and constructive tone.
  * Use **bold formatting** to emphasize key terms, ratings, and important findings.
  * Ensure the final output is a coherent narrative, not a list of raw data points. The goal is to produce a human-readable report that provides actionable insights.
