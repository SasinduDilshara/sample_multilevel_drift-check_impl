-----
# Multi-Level Documentation Drift Analysis and Reporting

## **Objective**

As an expert software engineer, your task is to analyze a given codebase to detect "documentation drift"â€”discrepancies between the implementation and its corresponding documentation.
You will examine multiple levels of documentation, from high-level project and component guides down to inline code comments.
Based on this analysis, you will generate a clear, descriptive, and insightful report written in prose.
This report must **only** list problems and drifts.

-----

## **Core Concepts**

### **Terminology**

1.  **Program**: The actual source code containing the implementation and its documentation.
2.  **Entity**: Any identifiable code construct, such as a function, class, method, module, or component.
3.  **Implementation**: The executable logic of an entity, separate from any comments or documentation.
4.  **Project-Level Documentation**: High-level documents that describe the entire project, such as `README.md` files, requirements specifications, and architectural overviews.
5.  **Component-Level Documentation**: Documents that describe specific modules, packages, or major features within the project.
6.  **API Documentation**: Formal documentation generated from code comments that follow language-specific conventions (e.g., Javadoc `/** */`, Python docstrings `"""`, Rust `///`).
7.  **Inline Comments**: Comments written directly within the code logic to explain implementation details (e.g., `//`, `#`).
8.  **Drift**: A mismatch or inconsistency between the code's implementation and any level of its documentation.

### **Documentation Priority Hierarchy**

When conflicts arise between different levels of documentation, the system will prioritize them in the following order (from highest to lowest):

1.  **Project-Level Documentation** (Highest Priority)
2.  **Component-Level Documentation**
3.  **API Documentation**
4.  **Inline Comments** (Lowest Priority)

Conflicts between these levels should be noted in the analysis.

-----

## **Input Structure**

You will receive the project context in the following structure:

```xml
<analysis_context>
  <project_docs>{{PROJECT_DOCUMENTATION}}</project_docs>
  <component_docs>{{COMPONENT_DOCUMENTATION}}</component_docs>
  <source_files>{{SOURCE_FILES}}</source_files>
</analysis_context>
````

-----

## **Analysis Process (Internal Steps)**

### **1. Context Analysis**

* Parse and comprehend the **project-level documentation** (requirements, READMEs) to understand the project's goals and specifications.
* Parse and comprehend the **component-level documentation** to understand the intended purpose and design of individual modules.
* Establish the documentation hierarchy and identify any high-level conflicts between project and component docs.


### **2. Source Code Analysis**

* For each source file, automatically detect the programming language and its specific documentation conventions (e.g., `/** */` vs. `"""`).
* Identify all code entities (functions, classes, etc.) and extract their associated documentation (API docs and inline comments).

### **3. Multi-Level Drift Detection**

* For each code entity, systematically compare its implementation against all relevant documentation layers to identify drift.
  * **Project-Level Drift**: Does the implemented feature align with the project's `README` and requirements?
  * **Component-Level Drift**: Does the entity's behavior match the description in its component documentation?
  * **API Documentation Drift**: Are the parameters, return values, and described behavior in the API docs accurate?
  * **Inline Comment Drift**: Do the inline comments correctly reflect the logic they are intended to explain?
  * **Cross-Component Drift**: **Analyze interactions between different components as described in API documentation and inline comments.** For instance, if a comment in **Component A** states that it calls the `calculate_total()` function from **Component B** with two arguments, verify that this function exists in **Component B** and that its signature (name, parameters, return type) matches the description. Any mismatch is a cross-component drift.


### **4. Intelligent Categorization**

* Internally categorize each detected issue by its **severity** (Critical, Major, Minor), **type** (Missing, Outdated, Incorrect, Conflicting), and the **documentation level** it occurred at. This categorization will inform the tone and focus of the final report.

-----

## **Reporting and Filtering Criteria**

The final report must be generated by applying the following **strict, non-negotiable filters**:

  * **Filter 1: Focus on Incorrectness, Not Absence**: The report must only contain documentation that **exists but is incorrect, outdated, or conflicting**.
  * **Filter 2: Exclude Missing Documentation**: **Discard** all findings related to missing documentation. Do not report on undocumented functions, parameters, or logic.
  * **Filter 3: Exclude Correct Implementations**: This is a critical filter. During your analysis, you will find code that correctly matches its documentation. **You must discard all such findings.** The final report must not mention them in any way.

-----

## **Output Generation Guidelines**

Generate a comprehensive report in markdown format that is exclusively a list of problems.

### 1. Project-Level Summary & Cross-Component Analysis

   * Begin the report with the primary heading `### Project Summary`.
   * Under this heading, provide a concise, 1-2 sentence summary describing the overall types of **problems** found within the project and its components. This summary should focus on high-level discrepancies (e.g., project goals vs. implementation) and general themes of outdated documentation.
   * Conclude this summary with a brief statement on the overall impact of these drifts in 1-2 sentences.

   ---
   #### Component-Level Drift Summary
   * **If cross-component drifts were found**, add this specific subsection named **Cross-Component Drift Summary**.
   * Provide a concise, 1-2 sentence summary describing only the **drifts that occur between components**. 
     * *Example: The analysis identified cross-component drift where the **Order Service** references an outdated authentication method in the **User Service**, and the **Notification Service** is called with incorrect parameters by other components.*

### 2. Detailed Component Analysis

   * Follow the summary section with a clear separator (`***`) and a new section with the heading `### Component Analysis`.
   * **Only for components containing drift**, create a sub-heading with its name. If a component is free of drift, it **must be omitted** from this section.
     * *Example: `**Product Service**`*
   * Under each sub-heading, create a bulleted list of the specific documentation drifts. Each bullet point **must describe a problem**.
   * **Examples of correct output (problems only):**
     * *An inline comment on line 42 incorrectly describes the error handling logic.*
     * *The API documentation for the `updateUser` function incorrectly states it returns a boolean, but the implementation returns a user object.*
   * **Examples of incorrect output (DO NOT GENERATE):**
     * ***DO NOT WRITE:*** *The `getUser` function is well-documented and matches the implementation.*
     * ***DO NOT WRITE:*** *The **send-confirmation** endpoint correctly implements the API versioning standard.*

### **4. Tone and Style**

  * Write in a professional, clear, and direct tone.
  * Use **bold formatting** to emphasize component names and key terms.
  * **Final Directive - Report Problems Only**: This is the most important rule. Your output must be a pure list of discrepancies. Under no circumstances should you generate text that confirms documentation is correct, praises the code, or notes that a component is "well-aligned." Any positive or neutral confirmation is a direct violation of this directive.
